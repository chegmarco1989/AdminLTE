{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chegmarco1989/AdminLTE/blob/master/Mini_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UliVFyPiRvdI",
        "outputId": "31e4e95c-2200-4d87-8429-6aac342d8180"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Aperçu du corpus : First Citizen:\n",
            "Before we proceed any further, hear …\n",
            "Taille du corpus  : 1115393\n",
            "Taille du vocab   : 65\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "from typing import List, Dict, Tuple\n",
        "import requests\n",
        "\n",
        "\n",
        "#choisir le corpus entre LaFontaine et Shakespeare en commentant la partie de code non voulue\n",
        "\"\"\"\n",
        "def load_corpus():\n",
        "    url = \"https://www.gutenberg.org/files/56327/56327-0.txt\"\n",
        "    text = requests.get(url).text\n",
        "    # Récupérer uniquement le texte entre les balises de début / fin\n",
        "    start = text.find(\"M DCCC LXVIII\")+len(\"M DCCC LXVIII\")+1\n",
        "    end = text.find(\"TABLE DES FABLES\")\n",
        "    if start == -1 or end == -1:\n",
        "        cleaned = text  # fallback : on garde tout\n",
        "    else:\n",
        "        cleaned = text[start:end]\n",
        "\n",
        "    # Mise en minuscules\n",
        "    cleaned = cleaned.lower()\n",
        "\n",
        "    # Suppression des annotations entre crochets (ex. [illustration], [16])\n",
        "    cleaned = re.sub(r\"\\[[^\\]]*\\]\", \" \", cleaned)\n",
        "\n",
        "    # Remplacement des retours‑ligne par des espaces simples\n",
        "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n",
        "\n",
        "    return cleaned\n",
        "    return text[start:end].lower()\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "def load_corpus() -> str:\n",
        "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "    text = requests.get(url, timeout=30).text\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "\n",
        "def build_vocab(corpus: str) -> Tuple[List[str], Dict[str, int], Dict[int, str]]:\n",
        "    \"\"\"Construit le vocabulaire et les tables d'indexation.\"\"\"\n",
        "    vocab = sorted(set(corpus))\n",
        "    stoi = {ch: i for i, ch in enumerate(vocab)}  # string → int\n",
        "    itos = {i: ch for ch, i in stoi.items()}      # int → string\n",
        "    return vocab, stoi, itos\n",
        "\n",
        "\n",
        "def encode(text: str, stoi: Dict[str, int]) -> List[int]:\n",
        "    \"\"\"Encode une chaîne en liste d'indices.\"\"\"\n",
        "    return [stoi[ch] for ch in text]\n",
        "\n",
        "\n",
        "def decode(indices: List[int], itos: Dict[int, str]) -> str:\n",
        "    \"\"\"Décode une liste d'indices en chaîne.\"\"\"\n",
        "    return \"\".join(itos[i] for i in indices)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    corpus = load_corpus()\n",
        "    vocab, stoi, itos = build_vocab(corpus)\n",
        "    encoded = encode(corpus, stoi)\n",
        "    print(\"Aperçu du corpus :\", corpus[:50], \"…\")\n",
        "    print(\"Taille du corpus  :\", len(corpus))\n",
        "    print(\"Taille du vocab   :\", len(vocab))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKKxWf9UjpPf"
      },
      "outputs": [],
      "source": [
        "from collections import Counter, defaultdict\n",
        "from typing import List, Dict, Tuple\n",
        "import re\n",
        "\n",
        "\n",
        "class SimpleBPE:\n",
        "    \"\"\"Tokenizer BPE simplifié (caractère → sous-mots).\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.stoi: Dict[str, int] = {}\n",
        "        self.itos: Dict[int, str] = {}\n",
        "        self.vocab: List[str] = []\n",
        "        self.merges: List[Tuple[str, str]] = []  # historique des fusions\n",
        "        # Token spécial de fin de mot\n",
        "        self.EOW = \"</w>\"\n",
        "\n",
        "    def _word_to_symbols(self, word: str) -> Tuple[str, ...]:\n",
        "        \"\"\"Découpe un mot en symboles (caractères) + token de fin.\"\"\"\n",
        "        return tuple(list(word) + [self.EOW])\n",
        "\n",
        "    def _get_stats(self, corpus_sym: List[Tuple[str, ...]]) -> Counter:\n",
        "        \"\"\"Compte la fréquence des paires de symboles dans le corpus.\"\"\"\n",
        "        pairs = Counter()\n",
        "        for symbols in corpus_sym:\n",
        "            for i in range(len(symbols) - 1):\n",
        "                pairs[(symbols[i], symbols[i + 1])] += 1\n",
        "        return pairs\n",
        "\n",
        "    def train(self, corpus: str, vocab_size: int = 200) -> None:\n",
        "        \"\"\"Apprend les fusions BPE jusqu'à atteindre la taille de vocabulaire.\"\"\"\n",
        "        # Nettoyage basique : minuscules + espaces simples\n",
        "        corpus = re.sub(r\"\\s+\", \" \", corpus.lower().strip())\n",
        "        words = corpus.split(\" \")\n",
        "        # Corpus sous forme de tuples de symboles\n",
        "        corpus_sym = [self._word_to_symbols(w) for w in words if w]\n",
        "\n",
        "        # Vocab initial : tous les symboles uniques\n",
        "        symbol_counter = Counter(s for sym in corpus_sym for s in sym)\n",
        "        self.vocab = sorted(symbol_counter)\n",
        "\n",
        "        def merge_pair(sym_seq: Tuple[str, ...], pair: Tuple[str, str]) -> Tuple[str, ...]:\n",
        "            \"\"\"Fusionne pair dans la séquence.\"\"\"\n",
        "            merged = []\n",
        "            i = 0\n",
        "            while i < len(sym_seq):\n",
        "                if i < len(sym_seq) - 1 and (sym_seq[i], sym_seq[i + 1]) == pair:\n",
        "                    merged.append(sym_seq[i] + sym_seq[i + 1])\n",
        "                    i += 2\n",
        "                else:\n",
        "                    merged.append(sym_seq[i])\n",
        "                    i += 1\n",
        "            return tuple(merged)\n",
        "\n",
        "        # Boucle principale\n",
        "        while len(self.vocab) < vocab_size:\n",
        "            stats = self._get_stats(corpus_sym)\n",
        "            if not stats:\n",
        "                break  # plus rien à fusionner\n",
        "            best_pair = stats.most_common(1)[0][0]\n",
        "            self.merges.append(best_pair)\n",
        "            # Mettre à jour corpus\n",
        "            corpus_sym = [merge_pair(sym, best_pair) for sym in corpus_sym]\n",
        "            # Mettre à jour vocab\n",
        "            new_symbol = best_pair[0] + best_pair[1]\n",
        "            self.vocab.append(new_symbol)\n",
        "\n",
        "        # Construire tables stoi/itos définitives\n",
        "        self.stoi = {sym: i for i, sym in enumerate(self.vocab)}\n",
        "        self.itos = {i: sym for sym, i in self.stoi.items()}\n",
        "\n",
        "    def _apply_merges(self, word: str) -> List[str]:\n",
        "        symbols = list(word) + [self.EOW]\n",
        "        for a, b in self.merges:\n",
        "            i = 0\n",
        "            while i < len(symbols) - 1:\n",
        "                if symbols[i] == a and symbols[i + 1] == b:\n",
        "                    symbols[i : i + 2] = [a + b]\n",
        "                else:\n",
        "                    i += 1\n",
        "        return symbols\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        \"\"\"Encode une chaîne en liste d'indices (BPE).\"\"\"\n",
        "        tokens: List[int] = []\n",
        "        for word in re.sub(r\"\\s+\", \" \", text.lower().strip()).split(\" \"):\n",
        "            for sym in self._apply_merges(word):\n",
        "                if sym not in self.stoi:\n",
        "                    # Si symbole OOV, le décomposer caractère par caractère\n",
        "                    for ch in list(sym):\n",
        "                        tokens.append(self.stoi.get(ch, self.stoi[self.EOW]))\n",
        "                else:\n",
        "                    tokens.append(self.stoi[sym])\n",
        "        return tokens\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        \"\"\"Décode une liste d'indices en texte (sans les tokens </w>).\"\"\"\n",
        "        words: List[str] = []\n",
        "        current = []\n",
        "        for idx in ids:\n",
        "            sym = self.itos.get(idx, \"?\")\n",
        "            if sym == self.EOW:\n",
        "                words.append(\"\".join(current))\n",
        "                current = []\n",
        "            elif sym.endswith(self.EOW):\n",
        "                current.append(sym[:-len(self.EOW)])\n",
        "                words.append(\"\".join(current))\n",
        "                current = []\n",
        "            else:\n",
        "                current.append(sym)\n",
        "        if current:\n",
        "            words.append(\"\".join(current))\n",
        "        return \" \".join(words).strip()\n",
        "\n",
        "\n",
        "    def vocab_size(self) -> int:\n",
        "        return len(self.vocab)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.vocab_size()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1qfwzGcRowx",
        "outputId": "1b05a81d-1410-40a0-bc82-656abffb72ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[86, 99, 19, 73, 22, 106, 73] le chat dort\n"
          ]
        }
      ],
      "source": [
        "corpus = load_corpus()\n",
        "tok = SimpleBPE()\n",
        "tok.train(corpus, vocab_size=120)\n",
        "\n",
        "ids = tok.encode(\"Le chat dort\")\n",
        "print(ids, tok.decode(ids))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfG_Qr3VTNkd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def _generate_square_subsequent_mask(t: int, device: torch.device) -> torch.Tensor:\n",
        "    return torch.triu(torch.ones((t, t), dtype=torch.bool, device=device), diagonal=1)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1) -> None:\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, 4 * d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * d_model, d_model),\n",
        "        )\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, attn_mask: torch.Tensor) -> torch.Tensor:\n",
        "        # Self‑attention\n",
        "        attn_out, _ = self.attn(x, x, x, attn_mask=attn_mask, need_weights=False)\n",
        "        x = self.ln1(x + self.drop(attn_out))\n",
        "        # Feed‑forward\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.ln2(x + self.drop(ff_out))\n",
        "        return x\n",
        "\n",
        "\n",
        "class MiniTransformer(nn.Module):\n",
        "    \"\"\"Transformer jouet auto‑régresseur avec `n_layers` blocs.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int = 128,\n",
        "        n_heads: int = 4,\n",
        "        n_layers: int = 2,\n",
        "        max_seq_len: int = 64,\n",
        "        dropout: float = 0.1,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        # Embeddings\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding(max_seq_len, d_model)\n",
        "\n",
        "        # Pile de blocs Transformer\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [TransformerBlock(d_model, n_heads, dropout) for _ in range(n_layers)]\n",
        "        )\n",
        "\n",
        "        # Projection finale\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"idx : (B, T) → logits : (B, T, vocab_size)\"\"\"\n",
        "        B, T = idx.shape\n",
        "        if T > self.max_seq_len:\n",
        "            raise ValueError(\n",
        "                f\"Sequence length {T} exceeds max_seq_len {self.max_seq_len}\"\n",
        "            )\n",
        "\n",
        "        # Embedding\n",
        "        token_emb = self.tok_emb(idx)\n",
        "        pos = torch.arange(T, device=idx.device).unsqueeze(0)\n",
        "        x = token_emb + self.pos_emb(pos)\n",
        "\n",
        "        # Masque causal\n",
        "        attn_mask = _generate_square_subsequent_mask(T, idx.device)\n",
        "\n",
        "        # Empiler les blocs\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x, attn_mask)\n",
        "\n",
        "        # Projection finale\n",
        "        logits = self.head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdTQjHzkXILZ",
        "outputId": "c31a3931-505b-4aff-91c2-f853ea637f73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🗂️  Chargement du corpus…\n",
            "🧩 Entraînement du tokenizer BPE…\n",
            "🪄 Encodage du corpus → ids…\n",
            "📏 Longueur jeu train : 326548 tokens\n",
            "📏 Longueur jeu valid : 36284 tokens\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from typing import Tuple\n",
        "\n",
        "def build_dataset(\n",
        "    corpus: str,\n",
        "    tokenizer: SimpleBPE,\n",
        "    train_ratio: float = 0.9,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Encode le corpus complet puis renvoie (train_ids, val_ids).\"\"\"\n",
        "    ids = torch.tensor(tokenizer.encode(corpus), dtype=torch.long)\n",
        "    split_idx = int(len(ids) * train_ratio)\n",
        "    return ids[:split_idx], ids[split_idx:]\n",
        "\n",
        "\n",
        "def get_batch(\n",
        "    data: torch.Tensor,\n",
        "    batch_size: int,\n",
        "    block_size: int,\n",
        "    device: torch.device,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Renvoie un lot (X, Y) aléatoire de forme (B, T).\"\"\"\n",
        "    if len(data) - block_size - 1 < 0:\n",
        "        raise ValueError(f\"Data length ({len(data)}) is too small for block_size ({block_size})\")\n",
        "    idx = torch.randint(0, len(data) - block_size - 1, (batch_size,))\n",
        "    x = torch.stack([data[i : i + block_size] for i in idx]).to(device)\n",
        "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in idx]).to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32        # Nombre de séquences par batch\n",
        "BLOCK_SIZE = 64        # Longueur de séquence (tokens) vue par le modèle\n",
        "ITERS       = 3000     # Itérations d'entraînement (ajustez si besoin)\n",
        "EVAL_INT    = 200      # Fréquence d'évaluation\n",
        "LR          = 1e-3     # Taux d'apprentissage\n",
        "\n",
        "\n",
        "print(\"🗂️  Chargement du corpus…\")\n",
        "corpus = load_corpus()\n",
        "\n",
        "print(\"🧩 Entraînement du tokenizer BPE…\")\n",
        "tok = SimpleBPE()\n",
        "tok.train(corpus, vocab_size=1000)\n",
        "\n",
        "print(\"🪄 Encodage du corpus → ids…\")\n",
        "train_ids, val_ids = build_dataset(corpus, tok)\n",
        "\n",
        "print(\"📏 Longueur jeu train :\", len(train_ids), \"tokens\")\n",
        "print(\"📏 Longueur jeu valid :\", len(val_ids), \"tokens\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqhktaNkhIuf",
        "outputId": "ab31e871-92ba-4683-c12e-2123f43fde3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Instanciation du modèle…\n",
            "📦  Paramètres du modèle : 660736\n"
          ]
        }
      ],
      "source": [
        "print(\"🧠 Instanciation du modèle…\")\n",
        "model = MiniTransformer(vocab_size=len(tok),n_heads=8)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"📦  Paramètres du modèle : {sum(p.numel() for p in model.parameters())}\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "\n",
        "def estimate_loss() -> Tuple[float, float]:\n",
        "    \"\"\"Calcule la perte moyenne sur train et val (sans grad).\"\"\"\n",
        "    model.eval()\n",
        "    losses = {}\n",
        "    with torch.no_grad():\n",
        "        for split, data in (\"train\", train_ids), (\"val\", val_ids):\n",
        "            loss_sum = 0.0\n",
        "            iters = 50  # nombre de mini‑lots pour l'estimation\n",
        "            for _ in range(iters):\n",
        "                x, y = get_batch(data, BATCH_SIZE, BLOCK_SIZE, device)\n",
        "                logits = model(x)\n",
        "                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "                loss_sum += loss.item()\n",
        "            losses[split] = loss_sum / iters\n",
        "    model.train()\n",
        "    return losses[\"train\"], losses[\"val\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "lKKCnmeYXYCy",
        "outputId": "5e1fb8ff-ff6e-4fb1-f497-48e3e364def8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Début entraînement…\n",
            "Step    1/3000  |  Train loss 6.998  |  Val loss 6.997\n",
            "Step  200/3000  |  Train loss 5.267  |  Val loss 5.493\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-3000615298.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(\"🚀 Début entraînement…\")\n",
        "for step in range(1, ITERS + 1):\n",
        "    xb, yb = get_batch(train_ids, BATCH_SIZE, BLOCK_SIZE, device)\n",
        "\n",
        "    logits = model(xb)\n",
        "    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % EVAL_INT == 0 or step == 1:\n",
        "        train_loss, val_loss = estimate_loss()\n",
        "        print(\n",
        "            f\"Step {step:4d}/{ITERS}  |  Train loss {train_loss:.3f}  |  Val loss {val_loss:.3f}\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsP_i2t-YxAm"
      },
      "outputs": [],
      "source": [
        "def generate(\n",
        "    model: MiniTransformer,\n",
        "    prompt: str,\n",
        "    max_new_tokens: int = 50,\n",
        "    temperature: float = 1.0,\n",
        ") -> str:\n",
        "    model.eval()\n",
        "    idx = torch.tensor(tok.encode(prompt), dtype=torch.long, device=device).unsqueeze(0)  # (1, T)\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -BLOCK_SIZE:]\n",
        "            logits = model(idx_cond)\n",
        "            next_logits = logits[:, -1, :] / temperature  # ← température ici\n",
        "            probs = torch.softmax(next_logits, dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat([idx, next_id], dim=1)\n",
        "    return tok.decode(idx[0].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG2IREBIYy1d",
        "outputId": "3943c6e6-6953-4043-bbb1-9a0f056428ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📝 Génération après entraînement :\n",
            "i am no man in a sicion; but i should to do so i change to patiest with you: and shall be your honour. if your son-shoulders and thrive it to your france the\n"
          ]
        }
      ],
      "source": [
        "prompt = \"I am\"\n",
        "print(\"\\n📝 Génération après entraînement :\")\n",
        "print(generate(model, prompt, max_new_tokens=50, temperature=0.8))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}